{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d73149",
   "metadata": {},
   "source": [
    "Install Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af4ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.14.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.1->torch) (69.2.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached torchaudio-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchaudio\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [torchaudio]0\u001b[0m [torchaudio]lver-cu12]2]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchaudio-2.7.1 triton-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Collecting torchvision\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2025.5.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl (7.3 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.3.1\n",
      "\u001b[2K    Uninstalling triton-3.3.1:\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1━━━━━\u001b[0m \u001b[32m 0/17\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12━━\u001b[0m \u001b[32m 0/17\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.30m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:0m \u001b[32m 0/17\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/17\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85━━\u001b[0m \u001b[32m 2/17\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m 4/17\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77━━━\u001b[0m \u001b[32m 5/17\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4━━━\u001b[0m \u001b[32m 6/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4━━━━━\u001b[0m \u001b[32m 6/17\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77m \u001b[32m 7/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:━━━━━━━━━━━━\u001b[0m \u001b[32m 7/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77[0m \u001b[32m 7/17\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77[0m \u001b[32m 8/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77━\u001b[0m \u001b[32m 8/17\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu120m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80[0m \u001b[32m 9/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80━\u001b[0m \u001b[32m 9/17\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1━━━━\u001b[0m \u001b[32m10/17\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:m━━━━━━━━━━━━━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2━━\u001b[0m \u001b[32m11/17\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17━━━━━\u001b[0m \u001b[32m12/17\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu1291m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m13/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\u001b[0m \u001b[32m13/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m13/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2━━\u001b[0m \u001b[32m13/17\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.1m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m14/17\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m15/17\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.6.0+cu124 torchvision-0.21.0+cu124 triton-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from datasets) (2.3.0)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.11/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached pyarrow-20.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.3 MB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Installing collected packages: xxhash, requests, pyarrow, fsspec, dill, multiprocess, datasets\n",
      "\u001b[2K  Attempting uninstall: requests━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/7\u001b[0m [xxhash]\n",
      "\u001b[2K    Found existing installation: requests 2.31.0 \u001b[32m0/7\u001b[0m [xxhash]\n",
      "\u001b[2K    Uninstalling requests-2.31.0:━━━━━━━━━━━\u001b[0m \u001b[32m0/7\u001b[0m [xxhash]\n",
      "\u001b[2K      Successfully uninstalled requests-2.31.00m \u001b[32m0/7\u001b[0m [xxhash]\n",
      "\u001b[2K  Attempting uninstall: fsspec[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [pyarrow]]\n",
      "\u001b[2K    Found existing installation: fsspec 2025.5.1━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [pyarrow]\n",
      "\u001b[2K    Uninstalling fsspec-2025.5.1:m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [pyarrow]\n",
      "\u001b[2K      Successfully uninstalled fsspec-2025.5.1━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/7\u001b[0m [pyarrow]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [datasets]6/7\u001b[0m [datasets]ess]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.1 requires torch==2.7.1, but you have torch 2.6.0+cu124 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 pyarrow-20.0.0 requests-2.32.4 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting peft\n",
      "  Using cached peft-0.15.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft) (2.6.0+cu124)\n",
      "Collecting transformers (from peft)\n",
      "  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from peft) (4.67.1)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting safetensors (from peft)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft) (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft) (4.14.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.4.26)\n",
      "Collecting huggingface_hub>=0.25.0 (from peft)\n",
      "  Using cached huggingface_hub-0.33.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers->peft)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers->peft)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub>=0.25.0->peft)\n",
      "  Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (879 bytes)\n",
      "Using cached peft-0.15.2-py3-none-any.whl (411 kB)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Using cached transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "Using cached huggingface_hub-0.33.0-py3-none-any.whl (514 kB)\n",
      "Using cached hf_xet-1.1.5-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "Installing collected packages: safetensors, regex, hf-xet, huggingface_hub, tokenizers, transformers, accelerate, peft\n",
      "\u001b[2K  Attempting uninstall: huggingface_hub━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [hf-xet]\n",
      "\u001b[2K    Found existing installation: huggingface-hub 0.26.2━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [hf-xet]\n",
      "\u001b[2K    Uninstalling huggingface-hub-0.26.2:━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/8\u001b[0m [hf-xet]\n",
      "\u001b[2K      Successfully uninstalled huggingface-hub-0.26.2━━━━━━━━━━━━━\u001b[0m \u001b[32m3/8\u001b[0m [huggingface_hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [peft][32m7/8\u001b[0m [peft]erate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.8.1 hf-xet-1.1.5 huggingface_hub-0.33.0 peft-0.15.2 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: transformers[torch] in /opt/conda/lib/python3.11/site-packages (4.52.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch<2.7,>=2.1 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (2.6.0+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (from transformers[torch]) (1.8.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers[torch]) (1.1.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch<2.7,>=2.1->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch<2.7,>=2.1->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch<2.7,>=2.1->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers[torch]) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/lib/python3.11/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (1.1.5)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.4.26)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.11/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (4.58.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.21 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.11/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.11/site-packages (from evaluate) (0.33.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from evaluate) (24.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install torch torchaudio\n",
    "%pip install torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "%pip install pandas\n",
    "%pip install datasets\n",
    "%pip install peft\n",
    "%pip install transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "%pip install evaluate\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295d4f7",
   "metadata": {},
   "source": [
    "Import Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd48f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob,math, torch, time, evaluate, pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42ee5c",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17661afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA                    = False\n",
    "DATA_DIR                    = \"outputs/github-ts-output-formatted\"\n",
    "METADATA_CSV                = \"outputs/github-ts-output-types.csv\"\n",
    "OUTPUT_DIR                  = \"outputs/typescriptmate-500000\"\n",
    "BATCH_SIZE                  = 4\n",
    "MAX_LENGTH                  = 512\n",
    "EPOCHS                      = 5\n",
    "LR                          = 5e-5\n",
    "GRAD_CLIP                   = 0.0\n",
    "SEED                        = 42\n",
    "WEIGHT_DECAY                = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be249c4",
   "metadata": {},
   "source": [
    "Count number of TypeScipt files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9a000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 379349\n"
     ]
    }
   ],
   "source": [
    "file_count = sum(len(files) for _, _, files in os.walk(DATA_DIR))\n",
    "print(\"Total files:\", file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93bf93",
   "metadata": {},
   "source": [
    "Check if MPS (Accelerated PyTorch Training for Apple Silicon) is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cea423c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762da94",
   "metadata": {},
   "source": [
    "Load metadata for Type Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "337e1a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 378001 metadata rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(METADATA_CSV)\n",
    "metadata_cols = [\n",
    "    \"Interfaces\", \"TypeAliases\", \"Enums\",\n",
    "    \"Classes\", \"Decorators\", \"Imports\",\n",
    "    \"Exports\", \"PredefinedTypesUsed\"\n",
    "]\n",
    "for col in metadata_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "print(f\"Loaded {len(df)} metadata rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0328c5",
   "metadata": {},
   "source": [
    "Attach file text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f6cbbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64812b597f84e6dbbb665724e561650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/378001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['File', 'Interfaces', 'TypeAliases', 'Enums', 'Classes', 'Decorators', 'Imports', 'Exports', 'PredefinedTypesUsed', 'text', 'interfaces', 'type_aliases', 'enums', 'classes', 'decorators', 'imports', 'exports', 'predefined_types'],\n",
      "    num_rows: 378001\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_meta = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "def add_text_and_metadata(example):\n",
    "    path = example[\"File\"]\n",
    "    if not os.path.isabs(path):\n",
    "        path = os.path.join(DATA_DIR, path)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            example[\"text\"] = f.read()\n",
    "    except FileNotFoundError:\n",
    "        example[\"text\"] = \"\"\n",
    "\n",
    "    example[\"interfaces\"] = example.get(\"Interfaces\", \"\")\n",
    "    example[\"type_aliases\"] = example.get(\"TypeAliases\", \"\")\n",
    "    example[\"enums\"] = example.get(\"Enums\", \"\")\n",
    "    example[\"classes\"] = example.get(\"Classes\", \"\")\n",
    "    example[\"decorators\"] = example.get(\"Decorators\", \"\")\n",
    "    example[\"imports\"] = example.get(\"Imports\", \"\")\n",
    "    example[\"exports\"] = example.get(\"Exports\", \"\")\n",
    "    example[\"predefined_types\"] = example.get(\"PredefinedTypesUsed\", \"\")\n",
    "    return example\n",
    "\n",
    "dataset_meta = dataset_meta.map(add_text_and_metadata, batched=False)\n",
    "print(dataset_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652c646",
   "metadata": {},
   "source": [
    "Filter bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10e4ff74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ae5c36c3504770932edf52aa92d810",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/378001 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-empty examples: 377717\n"
     ]
    }
   ],
   "source": [
    "dataset_meta = dataset_meta.filter(lambda ex: ex[\"text\"].strip() != \"\")\n",
    "print(\"Non-empty examples:\", len(dataset_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383882f4",
   "metadata": {},
   "source": [
    "Split and filter train and validation data for annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d09ff80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ec07f11b1a4ee38ad8f8e039f215c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/339945 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "120331a7ae614509aa128f206828a99b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/37772 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered split:\n",
      "  • train: 77183\n",
      "  • validation: 8606\n"
     ]
    }
   ],
   "source": [
    "splits = dataset_meta.train_test_split(test_size=0.1, seed=SEED)\n",
    "datasets = DatasetDict({\n",
    "    \"train\": splits[\"train\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"]),\n",
    "    \"validation\": splits[\"test\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"])\n",
    "})\n",
    "print(\"Filtered split:\")\n",
    "print(\"  • train:\", len(datasets[\"train\"]))\n",
    "print(\"  • validation:\", len(datasets[\"validation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feff32a",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1992937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a9842643934516905459ea87f49636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/77183 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d961fcbea7c446a936adcc83b696b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized = datasets.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be67ed",
   "metadata": {},
   "source": [
    "Sanity check on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7a48422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usable tokenized examples: 77183 / 77183\n"
     ]
    }
   ],
   "source": [
    "valid_count = sum(\n",
    "    any(tok != tokenizer.eos_token_id for tok in ex[\"input_ids\"])\n",
    "    for ex in tokenized[\"train\"]\n",
    ")\n",
    "print(f\"Usable tokenized examples: {valid_count} / {len(tokenized['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac802f2",
   "metadata": {},
   "source": [
    "Collator & base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9086823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81570dfb",
   "metadata": {},
   "source": [
    "Apply LoRA if enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3516eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=338,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        target_modules=[\"c_attn\", \"q_attn\", \"v_attn\"]\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    model = base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e611b95",
   "metadata": {},
   "source": [
    "Move model to supported device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dea532f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10483e34",
   "metadata": {},
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    max_grad_norm=GRAD_CLIP,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8870",
   "metadata": {},
   "source": [
    "Trainer with loss logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1050a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.17, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "class LossLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"LOGS:\", logs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[LossLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f50e1",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074661a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20087' max='48240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20087/48240 3:58:28 < 5:34:15, 1.40 it/s, Epoch 2.08/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.801100</td>\n",
       "      <td>1.653206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.678000</td>\n",
       "      <td>1.555488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.594200</td>\n",
       "      <td>1.503915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.556800</td>\n",
       "      <td>1.467130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.559500</td>\n",
       "      <td>1.437223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.512500</td>\n",
       "      <td>1.411766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.480700</td>\n",
       "      <td>1.392683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.445100</td>\n",
       "      <td>1.376323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.427300</td>\n",
       "      <td>1.359899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.433000</td>\n",
       "      <td>1.346729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.411100</td>\n",
       "      <td>1.337225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.432800</td>\n",
       "      <td>1.324561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.380500</td>\n",
       "      <td>1.312339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.359700</td>\n",
       "      <td>1.305210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.393100</td>\n",
       "      <td>1.295169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.317400</td>\n",
       "      <td>1.285988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.352900</td>\n",
       "      <td>1.276357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.369300</td>\n",
       "      <td>1.272783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.340600</td>\n",
       "      <td>1.264981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.348900</td>\n",
       "      <td>1.257773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.314700</td>\n",
       "      <td>1.253833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.310400</td>\n",
       "      <td>1.247083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.282200</td>\n",
       "      <td>1.244202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.295400</td>\n",
       "      <td>1.239562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.258800</td>\n",
       "      <td>1.231572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.274200</td>\n",
       "      <td>1.227619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.262400</td>\n",
       "      <td>1.224975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.229500</td>\n",
       "      <td>1.219306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.245600</td>\n",
       "      <td>1.214750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.242900</td>\n",
       "      <td>1.211644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.266300</td>\n",
       "      <td>1.207711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.245800</td>\n",
       "      <td>1.203665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.279800</td>\n",
       "      <td>1.201948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.259900</td>\n",
       "      <td>1.199477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.210100</td>\n",
       "      <td>1.193709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.238500</td>\n",
       "      <td>1.190214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.225700</td>\n",
       "      <td>1.188475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.230300</td>\n",
       "      <td>1.185778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.211900</td>\n",
       "      <td>1.183623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.230300</td>\n",
       "      <td>1.179056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGS: {'loss': 2.219, 'learning_rate': 4.989738805970149e-05, 'epoch': 0.010364842454394693}\n",
      "LOGS: {'loss': 1.9512, 'learning_rate': 4.9793739635157546e-05, 'epoch': 0.020729684908789386}\n",
      "LOGS: {'loss': 1.8667, 'learning_rate': 4.96900912106136e-05, 'epoch': 0.03109452736318408}\n",
      "LOGS: {'loss': 1.7901, 'learning_rate': 4.958644278606965e-05, 'epoch': 0.04145936981757877}\n",
      "LOGS: {'loss': 1.8011, 'learning_rate': 4.948279436152571e-05, 'epoch': 0.051824212271973466}\n",
      "LOGS: {'eval_loss': 1.6532058715820312, 'eval_runtime': 144.562, 'eval_samples_per_second': 59.532, 'eval_steps_per_second': 14.886, 'epoch': 0.051824212271973466}\n",
      "LOGS: {'loss': 1.7528, 'learning_rate': 4.937914593698176e-05, 'epoch': 0.06218905472636816}\n",
      "LOGS: {'loss': 1.7187, 'learning_rate': 4.9275497512437814e-05, 'epoch': 0.07255389718076286}\n",
      "LOGS: {'loss': 1.7019, 'learning_rate': 4.917184908789387e-05, 'epoch': 0.08291873963515754}\n",
      "LOGS: {'loss': 1.6759, 'learning_rate': 4.9068200663349915e-05, 'epoch': 0.09328358208955224}\n",
      "LOGS: {'loss': 1.678, 'learning_rate': 4.8964552238805975e-05, 'epoch': 0.10364842454394693}\n",
      "LOGS: {'eval_loss': 1.5554882287979126, 'eval_runtime': 144.8427, 'eval_samples_per_second': 59.416, 'eval_steps_per_second': 14.857, 'epoch': 0.10364842454394693}\n",
      "LOGS: {'loss': 1.6393, 'learning_rate': 4.886090381426203e-05, 'epoch': 0.11401326699834162}\n",
      "LOGS: {'loss': 1.6377, 'learning_rate': 4.8757255389718075e-05, 'epoch': 0.12437810945273632}\n",
      "LOGS: {'loss': 1.6579, 'learning_rate': 4.865360696517413e-05, 'epoch': 0.134742951907131}\n",
      "LOGS: {'loss': 1.6048, 'learning_rate': 4.854995854063019e-05, 'epoch': 0.1451077943615257}\n",
      "LOGS: {'loss': 1.5942, 'learning_rate': 4.8446310116086236e-05, 'epoch': 0.15547263681592038}\n",
      "LOGS: {'eval_loss': 1.5039153099060059, 'eval_runtime': 144.9453, 'eval_samples_per_second': 59.374, 'eval_steps_per_second': 14.847, 'epoch': 0.15547263681592038}\n",
      "LOGS: {'loss': 1.6027, 'learning_rate': 4.834266169154229e-05, 'epoch': 0.16583747927031509}\n",
      "LOGS: {'loss': 1.5749, 'learning_rate': 4.823901326699834e-05, 'epoch': 0.1762023217247098}\n",
      "LOGS: {'loss': 1.5884, 'learning_rate': 4.81353648424544e-05, 'epoch': 0.1865671641791045}\n",
      "LOGS: {'loss': 1.589, 'learning_rate': 4.803171641791045e-05, 'epoch': 0.19693200663349916}\n",
      "LOGS: {'loss': 1.5568, 'learning_rate': 4.7928067993366504e-05, 'epoch': 0.20729684908789386}\n",
      "LOGS: {'eval_loss': 1.467130422592163, 'eval_runtime': 144.8512, 'eval_samples_per_second': 59.413, 'eval_steps_per_second': 14.857, 'epoch': 0.20729684908789386}\n",
      "LOGS: {'loss': 1.5483, 'learning_rate': 4.782441956882255e-05, 'epoch': 0.21766169154228857}\n",
      "LOGS: {'loss': 1.5387, 'learning_rate': 4.772077114427861e-05, 'epoch': 0.22802653399668324}\n",
      "LOGS: {'loss': 1.5337, 'learning_rate': 4.7617122719734665e-05, 'epoch': 0.23839137645107794}\n",
      "LOGS: {'loss': 1.5544, 'learning_rate': 4.751347429519071e-05, 'epoch': 0.24875621890547264}\n",
      "LOGS: {'loss': 1.5595, 'learning_rate': 4.7409825870646765e-05, 'epoch': 0.25912106135986734}\n",
      "LOGS: {'eval_loss': 1.4372228384017944, 'eval_runtime': 144.7151, 'eval_samples_per_second': 59.469, 'eval_steps_per_second': 14.871, 'epoch': 0.25912106135986734}\n",
      "LOGS: {'loss': 1.5047, 'learning_rate': 4.7306177446102826e-05, 'epoch': 0.269485903814262}\n",
      "LOGS: {'loss': 1.5346, 'learning_rate': 4.720252902155887e-05, 'epoch': 0.2798507462686567}\n",
      "LOGS: {'loss': 1.5176, 'learning_rate': 4.7098880597014926e-05, 'epoch': 0.2902155887230514}\n",
      "LOGS: {'loss': 1.5101, 'learning_rate': 4.699523217247098e-05, 'epoch': 0.3005804311774461}\n",
      "LOGS: {'loss': 1.5125, 'learning_rate': 4.689158374792703e-05, 'epoch': 0.31094527363184077}\n",
      "LOGS: {'eval_loss': 1.4117661714553833, 'eval_runtime': 144.7253, 'eval_samples_per_second': 59.464, 'eval_steps_per_second': 14.87, 'epoch': 0.31094527363184077}\n",
      "LOGS: {'loss': 1.4869, 'learning_rate': 4.678793532338309e-05, 'epoch': 0.3213101160862355}\n",
      "LOGS: {'loss': 1.5178, 'learning_rate': 4.668428689883914e-05, 'epoch': 0.33167495854063017}\n",
      "LOGS: {'loss': 1.4548, 'learning_rate': 4.6580638474295194e-05, 'epoch': 0.3420398009950249}\n",
      "LOGS: {'loss': 1.5008, 'learning_rate': 4.647699004975125e-05, 'epoch': 0.3524046434494196}\n",
      "LOGS: {'loss': 1.4807, 'learning_rate': 4.63733416252073e-05, 'epoch': 0.36276948590381425}\n",
      "LOGS: {'eval_loss': 1.3926829099655151, 'eval_runtime': 144.8499, 'eval_samples_per_second': 59.413, 'eval_steps_per_second': 14.857, 'epoch': 0.36276948590381425}\n",
      "LOGS: {'loss': 1.4643, 'learning_rate': 4.626969320066335e-05, 'epoch': 0.373134328358209}\n",
      "LOGS: {'loss': 1.4621, 'learning_rate': 4.61660447761194e-05, 'epoch': 0.38349917081260365}\n",
      "LOGS: {'loss': 1.4429, 'learning_rate': 4.606239635157546e-05, 'epoch': 0.3938640132669983}\n",
      "LOGS: {'loss': 1.4421, 'learning_rate': 4.595874792703151e-05, 'epoch': 0.40422885572139305}\n",
      "LOGS: {'loss': 1.4451, 'learning_rate': 4.585509950248756e-05, 'epoch': 0.41459369817578773}\n",
      "LOGS: {'eval_loss': 1.3763225078582764, 'eval_runtime': 144.7687, 'eval_samples_per_second': 59.447, 'eval_steps_per_second': 14.865, 'epoch': 0.41459369817578773}\n",
      "LOGS: {'loss': 1.4513, 'learning_rate': 4.5751451077943616e-05, 'epoch': 0.4249585406301824}\n",
      "LOGS: {'loss': 1.4345, 'learning_rate': 4.564780265339967e-05, 'epoch': 0.43532338308457713}\n",
      "LOGS: {'loss': 1.4354, 'learning_rate': 4.554415422885572e-05, 'epoch': 0.4456882255389718}\n",
      "LOGS: {'loss': 1.4277, 'learning_rate': 4.544050580431178e-05, 'epoch': 0.4560530679933665}\n",
      "LOGS: {'loss': 1.4273, 'learning_rate': 4.533685737976783e-05, 'epoch': 0.4664179104477612}\n",
      "LOGS: {'eval_loss': 1.359898567199707, 'eval_runtime': 144.7867, 'eval_samples_per_second': 59.439, 'eval_steps_per_second': 14.863, 'epoch': 0.4664179104477612}\n",
      "LOGS: {'loss': 1.4322, 'learning_rate': 4.5233208955223884e-05, 'epoch': 0.4767827529021559}\n",
      "LOGS: {'loss': 1.444, 'learning_rate': 4.512956053067994e-05, 'epoch': 0.48714759535655056}\n",
      "LOGS: {'loss': 1.4479, 'learning_rate': 4.5025912106135984e-05, 'epoch': 0.4975124378109453}\n",
      "LOGS: {'loss': 1.4358, 'learning_rate': 4.4922263681592045e-05, 'epoch': 0.50787728026534}\n",
      "LOGS: {'loss': 1.433, 'learning_rate': 4.48186152570481e-05, 'epoch': 0.5182421227197347}\n",
      "LOGS: {'eval_loss': 1.346729040145874, 'eval_runtime': 144.935, 'eval_samples_per_second': 59.378, 'eval_steps_per_second': 14.848, 'epoch': 0.5182421227197347}\n",
      "LOGS: {'loss': 1.3989, 'learning_rate': 4.4714966832504145e-05, 'epoch': 0.5286069651741293}\n",
      "LOGS: {'loss': 1.4269, 'learning_rate': 4.46113184079602e-05, 'epoch': 0.538971807628524}\n",
      "LOGS: {'loss': 1.4309, 'learning_rate': 4.450766998341626e-05, 'epoch': 0.5493366500829188}\n",
      "LOGS: {'loss': 1.3825, 'learning_rate': 4.4404021558872306e-05, 'epoch': 0.5597014925373134}\n",
      "LOGS: {'loss': 1.4111, 'learning_rate': 4.430037313432836e-05, 'epoch': 0.5700663349917081}\n",
      "LOGS: {'eval_loss': 1.337225317955017, 'eval_runtime': 144.7575, 'eval_samples_per_second': 59.451, 'eval_steps_per_second': 14.866, 'epoch': 0.5700663349917081}\n",
      "LOGS: {'loss': 1.4402, 'learning_rate': 4.419672470978441e-05, 'epoch': 0.5804311774461028}\n",
      "LOGS: {'loss': 1.4314, 'learning_rate': 4.409307628524047e-05, 'epoch': 0.5907960199004975}\n",
      "LOGS: {'loss': 1.4064, 'learning_rate': 4.398942786069652e-05, 'epoch': 0.6011608623548922}\n",
      "LOGS: {'loss': 1.4159, 'learning_rate': 4.3885779436152574e-05, 'epoch': 0.6115257048092869}\n",
      "LOGS: {'loss': 1.4328, 'learning_rate': 4.378213101160862e-05, 'epoch': 0.6218905472636815}\n",
      "LOGS: {'eval_loss': 1.3245609998703003, 'eval_runtime': 144.8253, 'eval_samples_per_second': 59.423, 'eval_steps_per_second': 14.859, 'epoch': 0.6218905472636815}\n",
      "LOGS: {'loss': 1.414, 'learning_rate': 4.367848258706468e-05, 'epoch': 0.6322553897180763}\n",
      "LOGS: {'loss': 1.3828, 'learning_rate': 4.3574834162520735e-05, 'epoch': 0.642620232172471}\n",
      "LOGS: {'loss': 1.3672, 'learning_rate': 4.347118573797678e-05, 'epoch': 0.6529850746268657}\n",
      "LOGS: {'loss': 1.4012, 'learning_rate': 4.3367537313432835e-05, 'epoch': 0.6633499170812603}\n",
      "LOGS: {'loss': 1.3805, 'learning_rate': 4.3263888888888895e-05, 'epoch': 0.6737147595356551}\n",
      "LOGS: {'eval_loss': 1.3123388290405273, 'eval_runtime': 144.808, 'eval_samples_per_second': 59.43, 'eval_steps_per_second': 14.861, 'epoch': 0.6737147595356551}\n",
      "LOGS: {'loss': 1.3666, 'learning_rate': 4.316024046434494e-05, 'epoch': 0.6840796019900498}\n",
      "LOGS: {'loss': 1.348, 'learning_rate': 4.3056592039800996e-05, 'epoch': 0.6944444444444444}\n",
      "LOGS: {'loss': 1.4052, 'learning_rate': 4.295294361525705e-05, 'epoch': 0.7048092868988391}\n",
      "LOGS: {'loss': 1.383, 'learning_rate': 4.28492951907131e-05, 'epoch': 0.7151741293532339}\n",
      "LOGS: {'loss': 1.3597, 'learning_rate': 4.274564676616916e-05, 'epoch': 0.7255389718076285}\n",
      "LOGS: {'eval_loss': 1.3052095174789429, 'eval_runtime': 144.8062, 'eval_samples_per_second': 59.431, 'eval_steps_per_second': 14.861, 'epoch': 0.7255389718076285}\n",
      "LOGS: {'loss': 1.3838, 'learning_rate': 4.264199834162521e-05, 'epoch': 0.7359038142620232}\n",
      "LOGS: {'loss': 1.4014, 'learning_rate': 4.2538349917081264e-05, 'epoch': 0.746268656716418}\n",
      "LOGS: {'loss': 1.365, 'learning_rate': 4.243470149253732e-05, 'epoch': 0.7566334991708126}\n",
      "LOGS: {'loss': 1.3732, 'learning_rate': 4.233105306799337e-05, 'epoch': 0.7669983416252073}\n",
      "LOGS: {'loss': 1.3931, 'learning_rate': 4.222740464344942e-05, 'epoch': 0.777363184079602}\n",
      "LOGS: {'eval_loss': 1.2951687574386597, 'eval_runtime': 144.8041, 'eval_samples_per_second': 59.432, 'eval_steps_per_second': 14.861, 'epoch': 0.777363184079602}\n",
      "LOGS: {'loss': 1.3602, 'learning_rate': 4.212375621890547e-05, 'epoch': 0.7877280265339967}\n",
      "LOGS: {'loss': 1.3822, 'learning_rate': 4.202010779436153e-05, 'epoch': 0.7980928689883914}\n",
      "LOGS: {'loss': 1.3826, 'learning_rate': 4.191645936981758e-05, 'epoch': 0.8084577114427861}\n",
      "LOGS: {'loss': 1.3567, 'learning_rate': 4.181281094527363e-05, 'epoch': 0.8188225538971807}\n",
      "LOGS: {'loss': 1.3174, 'learning_rate': 4.1709162520729686e-05, 'epoch': 0.8291873963515755}\n",
      "LOGS: {'eval_loss': 1.2859877347946167, 'eval_runtime': 144.7535, 'eval_samples_per_second': 59.453, 'eval_steps_per_second': 14.867, 'epoch': 0.8291873963515755}\n",
      "LOGS: {'loss': 1.3806, 'learning_rate': 4.160551409618574e-05, 'epoch': 0.8395522388059702}\n",
      "LOGS: {'loss': 1.3376, 'learning_rate': 4.150186567164179e-05, 'epoch': 0.8499170812603648}\n",
      "LOGS: {'loss': 1.3324, 'learning_rate': 4.1398217247097847e-05, 'epoch': 0.8602819237147595}\n",
      "LOGS: {'loss': 1.3363, 'learning_rate': 4.12945688225539e-05, 'epoch': 0.8706467661691543}\n",
      "LOGS: {'loss': 1.3529, 'learning_rate': 4.1190920398009954e-05, 'epoch': 0.8810116086235489}\n",
      "LOGS: {'eval_loss': 1.2763572931289673, 'eval_runtime': 144.7613, 'eval_samples_per_second': 59.45, 'eval_steps_per_second': 14.866, 'epoch': 0.8810116086235489}\n",
      "LOGS: {'loss': 1.3549, 'learning_rate': 4.108727197346601e-05, 'epoch': 0.8913764510779436}\n",
      "LOGS: {'loss': 1.3601, 'learning_rate': 4.0983623548922054e-05, 'epoch': 0.9017412935323383}\n",
      "LOGS: {'loss': 1.3532, 'learning_rate': 4.0879975124378115e-05, 'epoch': 0.912106135986733}\n",
      "LOGS: {'loss': 1.3433, 'learning_rate': 4.077632669983417e-05, 'epoch': 0.9224709784411277}\n",
      "LOGS: {'loss': 1.3693, 'learning_rate': 4.0672678275290215e-05, 'epoch': 0.9328358208955224}\n",
      "LOGS: {'eval_loss': 1.2727829217910767, 'eval_runtime': 144.7845, 'eval_samples_per_second': 59.44, 'eval_steps_per_second': 14.863, 'epoch': 0.9328358208955224}\n",
      "LOGS: {'loss': 1.3312, 'learning_rate': 4.056902985074627e-05, 'epoch': 0.943200663349917}\n",
      "LOGS: {'loss': 1.3423, 'learning_rate': 4.046538142620233e-05, 'epoch': 0.9535655058043118}\n",
      "LOGS: {'loss': 1.3369, 'learning_rate': 4.0361733001658376e-05, 'epoch': 0.9639303482587065}\n",
      "LOGS: {'loss': 1.3454, 'learning_rate': 4.025808457711443e-05, 'epoch': 0.9742951907131011}\n",
      "LOGS: {'loss': 1.3406, 'learning_rate': 4.015443615257048e-05, 'epoch': 0.9846600331674958}\n",
      "LOGS: {'eval_loss': 1.2649807929992676, 'eval_runtime': 144.7071, 'eval_samples_per_second': 59.472, 'eval_steps_per_second': 14.871, 'epoch': 0.9846600331674958}\n",
      "LOGS: {'loss': 1.3311, 'learning_rate': 4.0050787728026537e-05, 'epoch': 0.9950248756218906}\n",
      "LOGS: {'loss': 1.2949, 'learning_rate': 3.994713930348259e-05, 'epoch': 1.0053897180762852}\n",
      "LOGS: {'loss': 1.2793, 'learning_rate': 3.9843490878938644e-05, 'epoch': 1.01575456053068}\n",
      "LOGS: {'loss': 1.295, 'learning_rate': 3.973984245439469e-05, 'epoch': 1.0261194029850746}\n",
      "LOGS: {'loss': 1.3489, 'learning_rate': 3.963619402985075e-05, 'epoch': 1.0364842454394694}\n",
      "LOGS: {'eval_loss': 1.2577728033065796, 'eval_runtime': 144.7648, 'eval_samples_per_second': 59.448, 'eval_steps_per_second': 14.865, 'epoch': 1.0364842454394694}\n",
      "LOGS: {'loss': 1.299, 'learning_rate': 3.9532545605306805e-05, 'epoch': 1.046849087893864}\n",
      "LOGS: {'loss': 1.2996, 'learning_rate': 3.942889718076285e-05, 'epoch': 1.0572139303482586}\n",
      "LOGS: {'loss': 1.3152, 'learning_rate': 3.9325248756218905e-05, 'epoch': 1.0675787728026533}\n",
      "LOGS: {'loss': 1.2815, 'learning_rate': 3.9221600331674965e-05, 'epoch': 1.077943615257048}\n",
      "LOGS: {'loss': 1.3147, 'learning_rate': 3.911795190713101e-05, 'epoch': 1.0883084577114428}\n",
      "LOGS: {'eval_loss': 1.2538334131240845, 'eval_runtime': 144.8235, 'eval_samples_per_second': 59.424, 'eval_steps_per_second': 14.859, 'epoch': 1.0883084577114428}\n",
      "LOGS: {'loss': 1.3214, 'learning_rate': 3.9014303482587066e-05, 'epoch': 1.0986733001658375}\n",
      "LOGS: {'loss': 1.3118, 'learning_rate': 3.891065505804312e-05, 'epoch': 1.1090381426202323}\n",
      "LOGS: {'loss': 1.2693, 'learning_rate': 3.880700663349917e-05, 'epoch': 1.1194029850746268}\n",
      "LOGS: {'loss': 1.2742, 'learning_rate': 3.8703358208955226e-05, 'epoch': 1.1297678275290215}\n",
      "LOGS: {'loss': 1.3104, 'learning_rate': 3.859970978441128e-05, 'epoch': 1.1401326699834162}\n",
      "LOGS: {'eval_loss': 1.247083306312561, 'eval_runtime': 144.7718, 'eval_samples_per_second': 59.445, 'eval_steps_per_second': 14.865, 'epoch': 1.1401326699834162}\n",
      "LOGS: {'loss': 1.3192, 'learning_rate': 3.8496061359867334e-05, 'epoch': 1.150497512437811}\n",
      "LOGS: {'loss': 1.3092, 'learning_rate': 3.839241293532339e-05, 'epoch': 1.1608623548922057}\n",
      "LOGS: {'loss': 1.3262, 'learning_rate': 3.828876451077944e-05, 'epoch': 1.1712271973466004}\n",
      "LOGS: {'loss': 1.3035, 'learning_rate': 3.818511608623549e-05, 'epoch': 1.1815920398009951}\n",
      "LOGS: {'loss': 1.2822, 'learning_rate': 3.808146766169154e-05, 'epoch': 1.1919568822553896}\n",
      "LOGS: {'eval_loss': 1.244201898574829, 'eval_runtime': 144.8271, 'eval_samples_per_second': 59.423, 'eval_steps_per_second': 14.859, 'epoch': 1.1919568822553896}\n",
      "LOGS: {'loss': 1.2832, 'learning_rate': 3.79778192371476e-05, 'epoch': 1.2023217247097844}\n",
      "LOGS: {'loss': 1.2411, 'learning_rate': 3.787417081260365e-05, 'epoch': 1.212686567164179}\n",
      "LOGS: {'loss': 1.2978, 'learning_rate': 3.77705223880597e-05, 'epoch': 1.2230514096185738}\n",
      "LOGS: {'loss': 1.2743, 'learning_rate': 3.7666873963515756e-05, 'epoch': 1.2334162520729686}\n",
      "LOGS: {'loss': 1.2954, 'learning_rate': 3.756322553897181e-05, 'epoch': 1.243781094527363}\n",
      "LOGS: {'eval_loss': 1.2395623922348022, 'eval_runtime': 144.7881, 'eval_samples_per_second': 59.439, 'eval_steps_per_second': 14.863, 'epoch': 1.243781094527363}\n",
      "LOGS: {'loss': 1.2867, 'learning_rate': 3.745957711442786e-05, 'epoch': 1.2541459369817578}\n",
      "LOGS: {'loss': 1.3048, 'learning_rate': 3.7355928689883916e-05, 'epoch': 1.2645107794361525}\n",
      "LOGS: {'loss': 1.2847, 'learning_rate': 3.725228026533997e-05, 'epoch': 1.2748756218905473}\n",
      "LOGS: {'loss': 1.2718, 'learning_rate': 3.7148631840796024e-05, 'epoch': 1.285240464344942}\n",
      "LOGS: {'loss': 1.2588, 'learning_rate': 3.704498341625208e-05, 'epoch': 1.2956053067993367}\n",
      "LOGS: {'eval_loss': 1.231571912765503, 'eval_runtime': 144.7577, 'eval_samples_per_second': 59.451, 'eval_steps_per_second': 14.866, 'epoch': 1.2956053067993367}\n",
      "LOGS: {'loss': 1.2672, 'learning_rate': 3.6941334991708124e-05, 'epoch': 1.3059701492537314}\n",
      "LOGS: {'loss': 1.2456, 'learning_rate': 3.6837686567164184e-05, 'epoch': 1.316334991708126}\n",
      "LOGS: {'loss': 1.2783, 'learning_rate': 3.673403814262024e-05, 'epoch': 1.3266998341625207}\n",
      "LOGS: {'loss': 1.3006, 'learning_rate': 3.6630389718076285e-05, 'epoch': 1.3370646766169154}\n",
      "LOGS: {'loss': 1.2742, 'learning_rate': 3.652674129353234e-05, 'epoch': 1.3474295190713101}\n",
      "LOGS: {'eval_loss': 1.2276192903518677, 'eval_runtime': 144.7527, 'eval_samples_per_second': 59.453, 'eval_steps_per_second': 14.867, 'epoch': 1.3474295190713101}\n",
      "LOGS: {'loss': 1.3016, 'learning_rate': 3.64230928689884e-05, 'epoch': 1.3577943615257049}\n",
      "LOGS: {'loss': 1.2601, 'learning_rate': 3.6319444444444446e-05, 'epoch': 1.3681592039800994}\n",
      "LOGS: {'loss': 1.2695, 'learning_rate': 3.62157960199005e-05, 'epoch': 1.378524046434494}\n",
      "LOGS: {'loss': 1.2955, 'learning_rate': 3.611214759535655e-05, 'epoch': 1.3888888888888888}\n",
      "LOGS: {'loss': 1.2624, 'learning_rate': 3.6008499170812606e-05, 'epoch': 1.3992537313432836}\n",
      "LOGS: {'eval_loss': 1.2249754667282104, 'eval_runtime': 144.8227, 'eval_samples_per_second': 59.424, 'eval_steps_per_second': 14.86, 'epoch': 1.3992537313432836}\n",
      "LOGS: {'loss': 1.2738, 'learning_rate': 3.590485074626866e-05, 'epoch': 1.4096185737976783}\n",
      "LOGS: {'loss': 1.248, 'learning_rate': 3.5801202321724714e-05, 'epoch': 1.419983416252073}\n",
      "LOGS: {'loss': 1.2831, 'learning_rate': 3.569755389718076e-05, 'epoch': 1.4303482587064678}\n",
      "LOGS: {'loss': 1.2625, 'learning_rate': 3.559390547263682e-05, 'epoch': 1.4407131011608625}\n",
      "LOGS: {'loss': 1.2295, 'learning_rate': 3.5490257048092874e-05, 'epoch': 1.451077943615257}\n",
      "LOGS: {'eval_loss': 1.219306230545044, 'eval_runtime': 144.8368, 'eval_samples_per_second': 59.419, 'eval_steps_per_second': 14.858, 'epoch': 1.451077943615257}\n",
      "LOGS: {'loss': 1.2625, 'learning_rate': 3.538660862354892e-05, 'epoch': 1.4614427860696517}\n",
      "LOGS: {'loss': 1.2866, 'learning_rate': 3.5282960199004975e-05, 'epoch': 1.4718076285240465}\n",
      "LOGS: {'loss': 1.2678, 'learning_rate': 3.5179311774461035e-05, 'epoch': 1.4821724709784412}\n",
      "LOGS: {'loss': 1.255, 'learning_rate': 3.507566334991708e-05, 'epoch': 1.4925373134328357}\n",
      "LOGS: {'loss': 1.2456, 'learning_rate': 3.4972014925373136e-05, 'epoch': 1.5029021558872304}\n",
      "LOGS: {'eval_loss': 1.2147495746612549, 'eval_runtime': 144.8041, 'eval_samples_per_second': 59.432, 'eval_steps_per_second': 14.861, 'epoch': 1.5029021558872304}\n",
      "LOGS: {'loss': 1.2554, 'learning_rate': 3.403917910447761e-05, 'epoch': 1.5961857379767828}\n",
      "LOGS: {'loss': 1.2663, 'learning_rate': 3.393553067993367e-05, 'epoch': 1.6065505804311775}\n",
      "LOGS: {'eval_loss': 1.2077112197875977, 'eval_runtime': 144.8207, 'eval_samples_per_second': 59.425, 'eval_steps_per_second': 14.86, 'epoch': 1.6065505804311775}\n",
      "LOGS: {'loss': 1.2403, 'learning_rate': 3.383188225538972e-05, 'epoch': 1.616915422885572}\n",
      "LOGS: {'loss': 1.2534, 'learning_rate': 3.372823383084577e-05, 'epoch': 1.6272802653399667}\n",
      "LOGS: {'loss': 1.2611, 'learning_rate': 3.3624585406301825e-05, 'epoch': 1.6376451077943615}\n",
      "LOGS: {'loss': 1.2461, 'learning_rate': 3.352093698175788e-05, 'epoch': 1.6480099502487562}\n",
      "LOGS: {'loss': 1.2458, 'learning_rate': 3.341728855721393e-05, 'epoch': 1.658374792703151}\n",
      "LOGS: {'eval_loss': 1.2036654949188232, 'eval_runtime': 144.8467, 'eval_samples_per_second': 59.415, 'eval_steps_per_second': 14.857, 'epoch': 1.658374792703151}\n",
      "LOGS: {'loss': 1.218, 'learning_rate': 3.3313640132669986e-05, 'epoch': 1.6687396351575456}\n",
      "LOGS: {'loss': 1.2468, 'learning_rate': 3.320999170812604e-05, 'epoch': 1.6791044776119404}\n",
      "LOGS: {'loss': 1.2193, 'learning_rate': 3.3106343283582093e-05, 'epoch': 1.689469320066335}\n",
      "LOGS: {'loss': 1.2568, 'learning_rate': 3.300269485903815e-05, 'epoch': 1.6998341625207298}\n",
      "LOGS: {'loss': 1.2798, 'learning_rate': 3.2899046434494194e-05, 'epoch': 1.7101990049751243}\n",
      "LOGS: {'eval_loss': 1.201947569847107, 'eval_runtime': 144.8011, 'eval_samples_per_second': 59.433, 'eval_steps_per_second': 14.862, 'epoch': 1.7101990049751243}\n",
      "LOGS: {'loss': 1.2511, 'learning_rate': 3.2795398009950254e-05, 'epoch': 1.720563847429519}\n",
      "LOGS: {'loss': 1.2426, 'learning_rate': 3.269174958540631e-05, 'epoch': 1.7309286898839138}\n",
      "LOGS: {'loss': 1.2269, 'learning_rate': 3.2588101160862355e-05, 'epoch': 1.7412935323383083}\n",
      "LOGS: {'loss': 1.2523, 'learning_rate': 3.248445273631841e-05, 'epoch': 1.751658374792703}\n",
      "LOGS: {'loss': 1.2599, 'learning_rate': 3.238080431177447e-05, 'epoch': 1.7620232172470978}\n",
      "LOGS: {'eval_loss': 1.1994768381118774, 'eval_runtime': 144.8058, 'eval_samples_per_second': 59.431, 'eval_steps_per_second': 14.861, 'epoch': 1.7620232172470978}\n",
      "LOGS: {'loss': 1.2431, 'learning_rate': 3.2277155887230515e-05, 'epoch': 1.7723880597014925}\n",
      "LOGS: {'loss': 1.2266, 'learning_rate': 3.217350746268657e-05, 'epoch': 1.7827529021558872}\n",
      "LOGS: {'loss': 1.2422, 'learning_rate': 3.206985903814262e-05, 'epoch': 1.793117744610282}\n",
      "LOGS: {'loss': 1.2293, 'learning_rate': 3.1966210613598676e-05, 'epoch': 1.8034825870646767}\n",
      "LOGS: {'loss': 1.2101, 'learning_rate': 3.186256218905473e-05, 'epoch': 1.8138474295190714}\n",
      "LOGS: {'eval_loss': 1.193709135055542, 'eval_runtime': 144.7244, 'eval_samples_per_second': 59.465, 'eval_steps_per_second': 14.87, 'epoch': 1.8138474295190714}\n",
      "LOGS: {'loss': 1.2293, 'learning_rate': 3.1758913764510783e-05, 'epoch': 1.8242122719734661}\n",
      "LOGS: {'loss': 1.2051, 'learning_rate': 3.165526533996683e-05, 'epoch': 1.8345771144278606}\n",
      "LOGS: {'loss': 1.2267, 'learning_rate': 3.155161691542289e-05, 'epoch': 1.8449419568822554}\n",
      "LOGS: {'loss': 1.2639, 'learning_rate': 3.144796849087894e-05, 'epoch': 1.85530679933665}\n",
      "LOGS: {'loss': 1.2385, 'learning_rate': 3.134432006633499e-05, 'epoch': 1.8656716417910446}\n",
      "LOGS: {'eval_loss': 1.190213680267334, 'eval_runtime': 144.7313, 'eval_samples_per_second': 59.462, 'eval_steps_per_second': 14.869, 'epoch': 1.8656716417910446}\n",
      "LOGS: {'loss': 1.2204, 'learning_rate': 3.1240671641791045e-05, 'epoch': 1.8760364842454393}\n",
      "LOGS: {'loss': 1.2429, 'learning_rate': 3.11370232172471e-05, 'epoch': 1.886401326699834}\n",
      "LOGS: {'loss': 1.2709, 'learning_rate': 3.103337479270315e-05, 'epoch': 1.8967661691542288}\n",
      "LOGS: {'loss': 1.2331, 'learning_rate': 3.0929726368159205e-05, 'epoch': 1.9071310116086235}\n",
      "LOGS: {'loss': 1.2257, 'learning_rate': 3.082607794361526e-05, 'epoch': 1.9174958540630183}\n",
      "LOGS: {'eval_loss': 1.1884753704071045, 'eval_runtime': 144.7242, 'eval_samples_per_second': 59.465, 'eval_steps_per_second': 14.87, 'epoch': 1.9174958540630183}\n",
      "LOGS: {'loss': 1.2635, 'learning_rate': 3.072242951907131e-05, 'epoch': 1.927860696517413}\n",
      "LOGS: {'loss': 1.2204, 'learning_rate': 3.0618781094527366e-05, 'epoch': 1.9382255389718077}\n",
      "LOGS: {'loss': 1.2285, 'learning_rate': 3.0515132669983416e-05, 'epoch': 1.9485903814262024}\n",
      "LOGS: {'loss': 1.2303, 'learning_rate': 3.041148424543947e-05, 'epoch': 1.9589552238805972}\n",
      "LOGS: {'loss': 1.2303, 'learning_rate': 3.0307835820895524e-05, 'epoch': 1.9693200663349917}\n",
      "LOGS: {'eval_loss': 1.185778021812439, 'eval_runtime': 144.7784, 'eval_samples_per_second': 59.443, 'eval_steps_per_second': 14.864, 'epoch': 1.9693200663349917}\n",
      "LOGS: {'loss': 1.2155, 'learning_rate': 3.0204187396351574e-05, 'epoch': 1.9796849087893864}\n",
      "LOGS: {'loss': 1.2355, 'learning_rate': 3.010053897180763e-05, 'epoch': 1.9900497512437811}\n",
      "LOGS: {'loss': 1.2521, 'learning_rate': 2.9996890547263684e-05, 'epoch': 2.0004145936981756}\n",
      "LOGS: {'loss': 1.2025, 'learning_rate': 2.9893242122719735e-05, 'epoch': 2.0107794361525704}\n",
      "LOGS: {'loss': 1.2119, 'learning_rate': 2.9789593698175788e-05, 'epoch': 2.021144278606965}\n",
      "LOGS: {'eval_loss': 1.18362295627594, 'eval_runtime': 144.7979, 'eval_samples_per_second': 59.435, 'eval_steps_per_second': 14.862, 'epoch': 2.021144278606965}\n",
      "LOGS: {'loss': 1.1935, 'learning_rate': 2.9685945273631842e-05, 'epoch': 2.03150912106136}\n",
      "LOGS: {'loss': 1.2163, 'learning_rate': 2.9582296849087892e-05, 'epoch': 2.0418739635157546}\n",
      "LOGS: {'loss': 1.2079, 'learning_rate': 2.947864842454395e-05, 'epoch': 2.0522388059701493}\n",
      "LOGS: {'loss': 1.2092, 'learning_rate': 2.9375000000000003e-05, 'epoch': 2.062603648424544}\n",
      "LOGS: {'loss': 1.2303, 'learning_rate': 2.9271351575456053e-05, 'epoch': 2.0729684908789388}\n",
      "LOGS: {'eval_loss': 1.1790555715560913, 'eval_runtime': 144.7639, 'eval_samples_per_second': 59.449, 'eval_steps_per_second': 14.866, 'epoch': 2.0729684908789388}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c5c14",
   "metadata": {},
   "source": [
    "Save model, tokenizer and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fff57",
   "metadata": {},
   "source": [
    "Basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Full eval results:\", eval_results)\n",
    "\n",
    "if eval_results.get(\"eval_loss\") is not None and not math.isnan(eval_results[\"eval_loss\"]):\n",
    "    print(\"Validation Perplexity: \", math.exp(eval_results[\"eval_loss\"]))\n",
    "else:\n",
    "    print(\"NaN eval loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c2ea0",
   "metadata": {},
   "source": [
    "Convert the trainer log history into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "df = pd.DataFrame(logs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88a7d3",
   "metadata": {},
   "source": [
    "Plot training loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff66623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"loss\"].notna()]\n",
    "plt.plot(train_df[\"step\"], train_df[\"loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f322c1",
   "metadata": {},
   "source": [
    "Plot training perplexity vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045acada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"perplexity\"] = train_df[\"loss\"].apply(math.exp)\n",
    "plt.figure()\n",
    "plt.plot(train_df[\"step\"], train_df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Perplexity\")\n",
    "plt.title(\"Training Perplexity over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec126838",
   "metadata": {},
   "source": [
    "Load each checkpoint’s weights into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_paths = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint-*\"))\n",
    "\n",
    "ckpt_paths = sorted(\n",
    "    ckpt_paths,\n",
    "    key=lambda p: int(re.search(r\"checkpoint-(\\d+)$\", p).group(1))\n",
    ")\n",
    "\n",
    "print(ckpt_paths)\n",
    "\n",
    "records = []\n",
    "for ckpt_path in ckpt_paths:\n",
    "    # 1) Reload the model weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(ckpt_path)\n",
    "    model.to(trainer.args.device)\n",
    "\n",
    "    # 2) Patch the Trainer’s model\n",
    "    trainer.model = model\n",
    "\n",
    "    # 3) Run evaluation on your validation split\n",
    "    metrics = trainer.evaluate()        # no args here\n",
    "\n",
    "    # 4) Record step & loss (& perplexity)\n",
    "    step = int(ckpt_path.split(\"-\")[-1])\n",
    "    loss = metrics[\"eval_loss\"]\n",
    "    records.append({\n",
    "      \"step\": step,\n",
    "      \"eval_loss\": loss,\n",
    "      \"perplexity\": math.exp(loss)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records).sort_values(\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b90f",
   "metadata": {},
   "source": [
    "Plot evaluation loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d515a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"eval_loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c972036",
   "metadata": {},
   "source": [
    "Convert loss to perplexity for easier interpretation: perplexity = exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.title(\"Validation Perplexity over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4b34b",
   "metadata": {},
   "source": [
    "Top-5 Token Accuracy and MRR over validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_topk_mrr(model, trainer, k=5, batch_size=1):\n",
    "    \"\"\"\n",
    "    Streams through the Trainer’s eval_dataloader batch-by-batch,\n",
    "    accumulates top-k matches and reciprocal ranks,\n",
    "    and keeps memory use small.\n",
    "    \"\"\"\n",
    "    # Put model in eval mode & grab device\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Use the Trainer’s built-in eval dataloader (with correct collation)\n",
    "    loader: DataLoader = trainer.get_eval_dataloader()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    topk_matches = 0\n",
    "    rr_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # batch is a dict of torch.Tensor already\n",
    "            labels = batch.pop(\"labels\").to(device)   # shape (bs, seq_len)\n",
    "            # move all other inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # forward\n",
    "            logits = model(**inputs).logits.cpu().numpy()  # (bs, seq_len, vocab_size)\n",
    "            lbls   = labels.cpu().numpy()                 # (bs, seq_len)\n",
    "\n",
    "            # mask out padding tokens\n",
    "            mask = lbls != -100                             # (bs, seq_len)\n",
    "            flat_logits = logits[mask].reshape(-1, logits.shape[-1])  # (N_toks, V)\n",
    "            flat_labels = lbls[mask].reshape(-1)                     # (N_toks,)\n",
    "\n",
    "            # Top-k matches via argpartition (cheap per-row)\n",
    "            topk_idxs = np.argpartition(flat_logits, -k, axis=-1)[:, -k:]\n",
    "            topk_matches += np.sum([flat_labels[i] in topk_idxs[i]\n",
    "                                    for i in range(flat_labels.shape[0])])\n",
    "\n",
    "            # MRR: rank = 1 + # of logits > true_logit\n",
    "            true_scores = flat_logits[np.arange(flat_labels.shape[0]), flat_labels]\n",
    "            ranks = 1 + np.sum(flat_logits > true_scores[:, None], axis=1)\n",
    "            rr_sum += np.sum(1.0 / ranks)\n",
    "\n",
    "            total_tokens += flat_labels.shape[0]\n",
    "\n",
    "    topk_acc = topk_matches / total_tokens\n",
    "    mrr      = rr_sum / total_tokens\n",
    "    return topk_acc, mrr\n",
    "\n",
    "# Usage:\n",
    "top5_acc, mrr = stream_topk_mrr(model, trainer, k=5)\n",
    "print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "print(f\"MRR:             {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adcad8",
   "metadata": {},
   "source": [
    "Top-5 Accuracy bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['Top-5 Accuracy'], [top5_acc])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Top-5 Token Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0f549",
   "metadata": {},
   "source": [
    "MRR bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['MRR'], [mrr])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Mean Reciprocal Rank (MRR)')\n",
    "plt.ylabel('MRR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c3f3d",
   "metadata": {},
   "source": [
    "Inference Latency Histogram\n",
    "Measure per-sample inference latency and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "loader = trainer.get_eval_dataloader()\n",
    "\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch.pop(\"labels\", None)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        start = time.monotonic()\n",
    "        _ = model(**inputs)\n",
    "        end = time.monotonic()\n",
    "        latencies.append(end - start)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "\n",
    "print(f\"Mean latency: {latencies.mean():.4f}s\")\n",
    "print(f\"Std  latency: {latencies.std():.4f}s\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(latencies, bins=20)\n",
    "plt.xlabel(\"Latency (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Inference Latency Distribution\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
