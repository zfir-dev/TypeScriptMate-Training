{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d73149",
   "metadata": {},
   "source": [
    "Install Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ab5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install torch torchvision torchaudio\n",
    "\n",
    "%pip install pandas\n",
    "%pip install datasets\n",
    "%pip install peft\n",
    "%pip install transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "%pip install evaluate\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295d4f7",
   "metadata": {},
   "source": [
    "Import Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, torch,time, evaluate, pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42ee5c",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17661afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA                    = False\n",
    "DATA_DIR                    = \"outputs/bigcode-ts-output-50000-formatted\"\n",
    "METADATA_CSV                = \"outputs/bigcode-ts-output-50000-types.csv\"\n",
    "OUTPUT_DIR                  = \"outputs/typescriptmate-50000\"\n",
    "BATCH_SIZE                  = 4\n",
    "MAX_LENGTH                  = 512\n",
    "EPOCHS                      = 5\n",
    "LR                          = 5e-5\n",
    "GRAD_CLIP                   = 0.0\n",
    "SEED                        = 42\n",
    "WEIGHT_DECAY                = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be249c4",
   "metadata": {},
   "source": [
    "Count number of TypeScipt files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count = sum(len(files) for _, _, files in os.walk(DATA_DIR))\n",
    "print(\"Total files:\", file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93bf93",
   "metadata": {},
   "source": [
    "Check if MPS (Accelerated PyTorch Training for Apple Silicon) is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762da94",
   "metadata": {},
   "source": [
    "Load metadata for Type Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_CSV)\n",
    "metadata_cols = [\n",
    "    \"Interfaces\", \"TypeAliases\", \"Enums\",\n",
    "    \"Classes\", \"Decorators\", \"Imports\",\n",
    "    \"Exports\", \"PredefinedTypesUsed\"\n",
    "]\n",
    "for col in metadata_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "print(f\"Loaded {len(df)} metadata rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0328c5",
   "metadata": {},
   "source": [
    "Attach file text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "def add_text_and_metadata(example):\n",
    "    path = example[\"File\"]\n",
    "    if not os.path.isabs(path):\n",
    "        path = os.path.join(DATA_DIR, path)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            example[\"text\"] = f.read()\n",
    "    except FileNotFoundError:\n",
    "        example[\"text\"] = \"\"\n",
    "\n",
    "    example[\"interfaces\"] = example.get(\"Interfaces\", \"\")\n",
    "    example[\"type_aliases\"] = example.get(\"TypeAliases\", \"\")\n",
    "    example[\"enums\"] = example.get(\"Enums\", \"\")\n",
    "    example[\"classes\"] = example.get(\"Classes\", \"\")\n",
    "    example[\"decorators\"] = example.get(\"Decorators\", \"\")\n",
    "    example[\"imports\"] = example.get(\"Imports\", \"\")\n",
    "    example[\"exports\"] = example.get(\"Exports\", \"\")\n",
    "    example[\"predefined_types\"] = example.get(\"PredefinedTypesUsed\", \"\")\n",
    "    return example\n",
    "\n",
    "dataset_meta = dataset_meta.map(add_text_and_metadata, batched=False)\n",
    "print(dataset_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652c646",
   "metadata": {},
   "source": [
    "Filter bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = dataset_meta.filter(lambda ex: ex[\"text\"].strip() != \"\")\n",
    "print(\"Non-empty examples:\", len(dataset_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383882f4",
   "metadata": {},
   "source": [
    "Split and filter train and validation data for annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ff80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset_meta.train_test_split(test_size=0.1, seed=SEED)\n",
    "datasets = DatasetDict({\n",
    "    \"train\": splits[\"train\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"]),\n",
    "    \"validation\": splits[\"test\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"])\n",
    "})\n",
    "print(\"Filtered split:\")\n",
    "print(\"  • train:\", len(datasets[\"train\"]))\n",
    "print(\"  • validation:\", len(datasets[\"validation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feff32a",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1992937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized = datasets.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be67ed",
   "metadata": {},
   "source": [
    "Sanity check on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a48422",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = sum(\n",
    "    any(tok != tokenizer.eos_token_id for tok in ex[\"input_ids\"])\n",
    "    for ex in tokenized[\"train\"]\n",
    ")\n",
    "print(f\"Usable tokenized examples: {valid_count} / {len(tokenized['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac802f2",
   "metadata": {},
   "source": [
    "Collator & base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81570dfb",
   "metadata": {},
   "source": [
    "Apply LoRA if enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=4,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.0,\n",
    "        target_modules=[\"c_attn\"]  # you can inspect model to try others\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    model = base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e611b95",
   "metadata": {},
   "source": [
    "Move model to supported device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10483e34",
   "metadata": {},
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    max_grad_norm=GRAD_CLIP,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8870",
   "metadata": {},
   "source": [
    "Trainer with loss logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1050a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"LOGS:\", logs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[LossLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f50e1",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c5c14",
   "metadata": {},
   "source": [
    "Save model, tokenizer and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fff57",
   "metadata": {},
   "source": [
    "Basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Full eval results:\", eval_results)\n",
    "\n",
    "if eval_results.get(\"eval_loss\") is not None and not math.isnan(eval_results[\"eval_loss\"]):\n",
    "    print(\"Validation Perplexity: \", math.exp(eval_results[\"eval_loss\"]))\n",
    "else:\n",
    "    print(\"NaN eval loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c2ea0",
   "metadata": {},
   "source": [
    "Convert the trainer log history into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "df = pd.DataFrame(logs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88a7d3",
   "metadata": {},
   "source": [
    "Plot training loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff66623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"loss\"].notna()]\n",
    "plt.plot(train_df[\"step\"], train_df[\"loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f322c1",
   "metadata": {},
   "source": [
    "Plot training perplexity vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045acada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"perplexity\"] = train_df[\"loss\"].apply(math.exp)\n",
    "plt.figure()\n",
    "plt.plot(train_df[\"step\"], train_df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Perplexity\")\n",
    "plt.title(\"Training Perplexity over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec126838",
   "metadata": {},
   "source": [
    "Load each checkpoint’s weights into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpts = [\n",
    "    \"checkpoint-500\",\"checkpoint-1000\",\"checkpoint-1500\",\n",
    "    \"checkpoint-2000\",\"checkpoint-2500\",\"checkpoint-3000\",\n",
    "    \"checkpoint-3500\",\"checkpoint-4000\",\"checkpoint-4010\"\n",
    "]\n",
    "\n",
    "# ckpts = [\n",
    "#     \"checkpoint-345\"\n",
    "# ]\n",
    "\n",
    "records = []\n",
    "for ckpt in ckpts:\n",
    "    ckpt_path = f\"{OUTPUT_DIR}/{ckpt}\"\n",
    "\n",
    "    # 1) Reload the model weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(ckpt_path)\n",
    "    model.to(trainer.args.device)\n",
    "\n",
    "    # 2) Patch the Trainer’s model\n",
    "    trainer.model = model\n",
    "\n",
    "    # 3) Run evaluation on your validation split\n",
    "    metrics = trainer.evaluate()        # no args here\n",
    "\n",
    "    # 4) Record step & loss (& perplexity)\n",
    "    step = int(ckpt.split(\"-\")[-1])\n",
    "    loss = metrics[\"eval_loss\"]\n",
    "    records.append({\n",
    "      \"step\": step,\n",
    "      \"eval_loss\": loss,\n",
    "      \"perplexity\": math.exp(loss)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records).sort_values(\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b90f",
   "metadata": {},
   "source": [
    "Plot evaluation loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d515a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"eval_loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c972036",
   "metadata": {},
   "source": [
    "Convert loss to perplexity for easier interpretation: perplexity = exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.title(\"Validation Perplexity over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4b34b",
   "metadata": {},
   "source": [
    "Top-5 Token Accuracy and MRR over validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_topk_mrr(model, trainer, k=5, batch_size=1):\n",
    "    \"\"\"\n",
    "    Streams through the Trainer’s eval_dataloader batch-by-batch,\n",
    "    accumulates top-k matches and reciprocal ranks,\n",
    "    and keeps memory use small.\n",
    "    \"\"\"\n",
    "    # Put model in eval mode & grab device\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Use the Trainer’s built-in eval dataloader (with correct collation)\n",
    "    loader: DataLoader = trainer.get_eval_dataloader()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    topk_matches = 0\n",
    "    rr_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # batch is a dict of torch.Tensor already\n",
    "            labels = batch.pop(\"labels\").to(device)   # shape (bs, seq_len)\n",
    "            # move all other inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # forward\n",
    "            logits = model(**inputs).logits.cpu().numpy()  # (bs, seq_len, vocab_size)\n",
    "            lbls   = labels.cpu().numpy()                 # (bs, seq_len)\n",
    "\n",
    "            # mask out padding tokens\n",
    "            mask = lbls != -100                             # (bs, seq_len)\n",
    "            flat_logits = logits[mask].reshape(-1, logits.shape[-1])  # (N_toks, V)\n",
    "            flat_labels = lbls[mask].reshape(-1)                     # (N_toks,)\n",
    "\n",
    "            # Top-k matches via argpartition (cheap per-row)\n",
    "            topk_idxs = np.argpartition(flat_logits, -k, axis=-1)[:, -k:]\n",
    "            topk_matches += np.sum([flat_labels[i] in topk_idxs[i]\n",
    "                                    for i in range(flat_labels.shape[0])])\n",
    "\n",
    "            # MRR: rank = 1 + # of logits > true_logit\n",
    "            true_scores = flat_logits[np.arange(flat_labels.shape[0]), flat_labels]\n",
    "            ranks = 1 + np.sum(flat_logits > true_scores[:, None], axis=1)\n",
    "            rr_sum += np.sum(1.0 / ranks)\n",
    "\n",
    "            total_tokens += flat_labels.shape[0]\n",
    "\n",
    "    topk_acc = topk_matches / total_tokens\n",
    "    mrr      = rr_sum / total_tokens\n",
    "    return topk_acc, mrr\n",
    "\n",
    "# Usage:\n",
    "top5_acc, mrr = stream_topk_mrr(model, trainer, k=5)\n",
    "print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "print(f\"MRR:             {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adcad8",
   "metadata": {},
   "source": [
    "Top-5 Accuracy bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['Top-5 Accuracy'], [top5_acc])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Top-5 Token Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0f549",
   "metadata": {},
   "source": [
    "MRR bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['MRR'], [mrr])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Mean Reciprocal Rank (MRR)')\n",
    "plt.ylabel('MRR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c3f3d",
   "metadata": {},
   "source": [
    "Inference Latency Histogram\n",
    "Measure per-sample inference latency and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "loader = trainer.get_eval_dataloader()\n",
    "\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch.pop(\"labels\", None)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        start = time.monotonic()\n",
    "        _ = model(**inputs)\n",
    "        end = time.monotonic()\n",
    "        latencies.append(end - start)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "\n",
    "print(f\"Mean latency: {latencies.mean():.4f}s\")\n",
    "print(f\"Std  latency: {latencies.std():.4f}s\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(latencies, bins=20)\n",
    "plt.xlabel(\"Latency (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Inference Latency Distribution\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation_apple",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
