{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d73149",
   "metadata": {},
   "source": [
    "Install Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af4ab5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.11/site-packages (25.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.6.0+cu124)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/lib/python3.11/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /opt/conda/lib/python3.11/site-packages (from torch) (1.11.1.6)\n",
      "Collecting triton==3.3.1 (from torch)\n",
      "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /opt/conda/lib/python3.11/site-packages (from triton==3.3.1->torch) (69.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
      "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
      "\u001b[2K  Attempting uninstall: triton━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: triton 3.2.0[0m \u001b[32m 0/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling triton-3.2.0:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 0/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled triton-3.2.0━\u001b[0m \u001b[32m 0/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: sympy 1.13.1━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling sympy-1.13.1:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.13.1━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.4.127[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.4.127:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.4.127━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.21.5━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.21.5:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.5.147\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.5.147:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.5.147━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127 \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.1270m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.1270m \u001b[32m 7/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.4.1270m \u001b[32m 8/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu120m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.4.5.8━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.4.5.8:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.3.1.1700m \u001b[32m10/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.3.1.170:━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu1291m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.1.3━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.1.3:m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.1.0.70━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.1.9:[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.6.0+cu124[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.6.0+cu124:━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.6.0+cu1241m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16/16\u001b[0m [torch]m15/16\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 triton-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.21.0+cu124)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Collecting torch==2.6.0 (from torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.5)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.6.0->torchvision) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/cu124/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Collecting triton==3.2.0 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch==2.6.0->torchvision)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Using cached https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl (768.5 MB)\n",
      "Using cached https://download.pytorch.org/whl/cu124/nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "Using cached https://download.pytorch.org/whl/triton-3.2.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (166.7 MB)\n",
      "Installing collected packages: triton, nvidia-cusparselt-cu12, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.3.1\n",
      "\u001b[2K    Uninstalling triton-3.3.1:\n",
      "\u001b[2K      Successfully uninstalled triton-3.3.1\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12━━\u001b[0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.30m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.3:0m \u001b[32m 0/16\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: sympy━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 1/16\u001b[0m [nvidia-cusparselt-cu12]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]parselt-cu12]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77━━━━━━━\u001b[0m \u001b[32m 2/16\u001b[0m [sympy]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85━━\u001b[0m \u001b[32m 3/16\u001b[0m [nvidia-nvtx-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.26.2━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.26.2:━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.26.2━━━━━━━━\u001b[0m \u001b[32m 4/16\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77━━━\u001b[0m \u001b[32m 5/16\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/16\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:━━━━━━━━━━━━\u001b[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77[0m \u001b[32m 7/16\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77━\u001b[0m \u001b[32m 8/16\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu1290m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80━\u001b[0m \u001b[32m 9/16\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1━━━━\u001b[0m \u001b[32m10/16\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:90m━━━━━━━━━━━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2━━\u001b[0m \u001b[32m11/16\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.5.1.17━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17━━━━━\u001b[0m \u001b[32m12/16\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2━━\u001b[0m \u001b[32m13/16\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.7.1[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m14/16\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.7.1:━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]olver-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.1━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m15/16\u001b[0m [torch]"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install torch torchaudio\n",
    "%pip install torchvision --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "%pip install pandas\n",
    "%pip install datasets\n",
    "%pip install peft\n",
    "%pip install transformers\n",
    "%pip install transformers[torch]\n",
    "%pip install 'accelerate>=0.26.0'\n",
    "\n",
    "%pip install matplotlib\n",
    "\n",
    "%pip install evaluate\n",
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a295d4f7",
   "metadata": {},
   "source": [
    "Import Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, torch,time, evaluate, pandas as pd, matplotlib.pyplot as plt, numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    GPT2TokenizerFast,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer, TrainingArguments, TrainerCallback\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d42ee5c",
   "metadata": {},
   "source": [
    "Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17661afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_LORA                    = True\n",
    "DATA_DIR                    = \"outputs/bigcode-ts-output-4000-formatted\"\n",
    "METADATA_CSV                = \"outputs/bigcode-ts-output-4000-types.csv\"\n",
    "OUTPUT_DIR                  = \"outputs/typescriptmate-4000-lora\"\n",
    "BATCH_SIZE                  = 4\n",
    "MAX_LENGTH                  = 512\n",
    "EPOCHS                      = 5\n",
    "LR                          = 5e-5\n",
    "GRAD_CLIP                   = 0.0\n",
    "SEED                        = 42\n",
    "WEIGHT_DECAY                = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be249c4",
   "metadata": {},
   "source": [
    "Count number of TypeScipt files in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a000d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count = sum(len(files) for _, _, files in os.walk(DATA_DIR))\n",
    "print(\"Total files:\", file_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa93bf93",
   "metadata": {},
   "source": [
    "Check if MPS (Accelerated PyTorch Training for Apple Silicon) is supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cea423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762da94",
   "metadata": {},
   "source": [
    "Load metadata for Type Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337e1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(METADATA_CSV)\n",
    "metadata_cols = [\n",
    "    \"Interfaces\", \"TypeAliases\", \"Enums\",\n",
    "    \"Classes\", \"Decorators\", \"Imports\",\n",
    "    \"Exports\", \"PredefinedTypesUsed\"\n",
    "]\n",
    "for col in metadata_cols:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(\"\")\n",
    "print(f\"Loaded {len(df)} metadata rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0328c5",
   "metadata": {},
   "source": [
    "Attach file text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6cbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "def add_text_and_metadata(example):\n",
    "    path = example[\"File\"]\n",
    "    if not os.path.isabs(path):\n",
    "        path = os.path.join(DATA_DIR, path)\n",
    "\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            example[\"text\"] = f.read()\n",
    "    except FileNotFoundError:\n",
    "        example[\"text\"] = \"\"\n",
    "\n",
    "    example[\"interfaces\"] = example.get(\"Interfaces\", \"\")\n",
    "    example[\"type_aliases\"] = example.get(\"TypeAliases\", \"\")\n",
    "    example[\"enums\"] = example.get(\"Enums\", \"\")\n",
    "    example[\"classes\"] = example.get(\"Classes\", \"\")\n",
    "    example[\"decorators\"] = example.get(\"Decorators\", \"\")\n",
    "    example[\"imports\"] = example.get(\"Imports\", \"\")\n",
    "    example[\"exports\"] = example.get(\"Exports\", \"\")\n",
    "    example[\"predefined_types\"] = example.get(\"PredefinedTypesUsed\", \"\")\n",
    "    return example\n",
    "\n",
    "dataset_meta = dataset_meta.map(add_text_and_metadata, batched=False)\n",
    "print(dataset_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652c646",
   "metadata": {},
   "source": [
    "Filter bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4ff74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = dataset_meta.filter(lambda ex: ex[\"text\"].strip() != \"\")\n",
    "print(\"Non-empty examples:\", len(dataset_meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383882f4",
   "metadata": {},
   "source": [
    "Split and filter train and validation data for annotated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ff80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = dataset_meta.train_test_split(test_size=0.1, seed=SEED)\n",
    "datasets = DatasetDict({\n",
    "    \"train\": splits[\"train\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"]),\n",
    "    \"validation\": splits[\"test\"].filter(lambda ex: ex[\"TypeAliases\"] or ex[\"Interfaces\"])\n",
    "})\n",
    "print(\"Filtered split:\")\n",
    "print(\"  • train:\", len(datasets[\"train\"]))\n",
    "print(\"  • validation:\", len(datasets[\"validation\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feff32a",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1992937",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "tokenized = datasets.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be67ed",
   "metadata": {},
   "source": [
    "Sanity check on tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a48422",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_count = sum(\n",
    "    any(tok != tokenizer.eos_token_id for tok in ex[\"input_ids\"])\n",
    "    for ex in tokenized[\"train\"]\n",
    ")\n",
    "print(f\"Usable tokenized examples: {valid_count} / {len(tokenized['train'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac802f2",
   "metadata": {},
   "source": [
    "Collator & base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    "    pad_to_multiple_of=None,\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "base_model.config.pad_token_id = base_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81570dfb",
   "metadata": {},
   "source": [
    "Apply LoRA if enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3516eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_LORA:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=338,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.0,\n",
    "        target_modules=[\"c_attn\"]  # you can inspect model to try others\n",
    "    )\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "else:\n",
    "    model = base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e611b95",
   "metadata": {},
   "source": [
    "Move model to supported device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = (\n",
    "    torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "    else torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10483e34",
   "metadata": {},
   "source": [
    "TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978576d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS, \n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY, \n",
    "    max_grad_norm=GRAD_CLIP,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8870",
   "metadata": {},
   "source": [
    "Trainer with loss logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1050a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(\"LOGS:\", logs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[LossLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650f50e1",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074661a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c5c14",
   "metadata": {},
   "source": [
    "Save model, tokenizer and deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ce6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fff57",
   "metadata": {},
   "source": [
    "Basic evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"Full eval results:\", eval_results)\n",
    "\n",
    "if eval_results.get(\"eval_loss\") is not None and not math.isnan(eval_results[\"eval_loss\"]):\n",
    "    print(\"Validation Perplexity: \", math.exp(eval_results[\"eval_loss\"]))\n",
    "else:\n",
    "    print(\"NaN eval loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0c2ea0",
   "metadata": {},
   "source": [
    "Convert the trainer log history into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d10db",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = trainer.state.log_history\n",
    "df = pd.DataFrame(logs)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d88a7d3",
   "metadata": {},
   "source": [
    "Plot training loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff66623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df[\"loss\"].notna()]\n",
    "plt.plot(train_df[\"step\"], train_df[\"loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Training Loss over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f322c1",
   "metadata": {},
   "source": [
    "Plot training perplexity vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045acada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"perplexity\"] = train_df[\"loss\"].apply(math.exp)\n",
    "plt.figure()\n",
    "plt.plot(train_df[\"step\"], train_df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Training Perplexity\")\n",
    "plt.title(\"Training Perplexity over Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec126838",
   "metadata": {},
   "source": [
    "Load each checkpoint’s weights into your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpts = [\n",
    "#     \"checkpoint-500\",\"checkpoint-1000\",\"checkpoint-1500\",\n",
    "#     \"checkpoint-2000\",\"checkpoint-2500\",\"checkpoint-3000\",\n",
    "#     \"checkpoint-3500\",\"checkpoint-4000\",\"checkpoint-4010\"\n",
    "# ]\n",
    "\n",
    "ckpts = [\n",
    "    \"checkpoint-350\"\n",
    "]\n",
    "\n",
    "records = []\n",
    "for ckpt in ckpts:\n",
    "    ckpt_path = f\"{OUTPUT_DIR}/{ckpt}\"\n",
    "\n",
    "    # 1) Reload the model weights\n",
    "    model = AutoModelForCausalLM.from_pretrained(ckpt_path)\n",
    "    model.to(trainer.args.device)\n",
    "\n",
    "    # 2) Patch the Trainer’s model\n",
    "    trainer.model = model\n",
    "\n",
    "    # 3) Run evaluation on your validation split\n",
    "    metrics = trainer.evaluate()        # no args here\n",
    "\n",
    "    # 4) Record step & loss (& perplexity)\n",
    "    step = int(ckpt.split(\"-\")[-1])\n",
    "    loss = metrics[\"eval_loss\"]\n",
    "    records.append({\n",
    "      \"step\": step,\n",
    "      \"eval_loss\": loss,\n",
    "      \"perplexity\": math.exp(loss)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records).sort_values(\"step\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8b90f",
   "metadata": {},
   "source": [
    "Plot evaluation loss vs global step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d515a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"eval_loss\"])\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.title(\"Validation Loss over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c972036",
   "metadata": {},
   "source": [
    "Convert loss to perplexity for easier interpretation: perplexity = exp(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df[\"step\"], df[\"perplexity\"], marker=\"o\")\n",
    "plt.xlabel(\"Global Step\")\n",
    "plt.ylabel(\"Validation Perplexity\")\n",
    "plt.title(\"Validation Perplexity over Checkpoints\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e4b34b",
   "metadata": {},
   "source": [
    "Top-5 Token Accuracy and MRR over validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edefddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_topk_mrr(model, trainer, k=5, batch_size=1):\n",
    "    \"\"\"\n",
    "    Streams through the Trainer’s eval_dataloader batch-by-batch,\n",
    "    accumulates top-k matches and reciprocal ranks,\n",
    "    and keeps memory use small.\n",
    "    \"\"\"\n",
    "    # Put model in eval mode & grab device\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Use the Trainer’s built-in eval dataloader (with correct collation)\n",
    "    loader: DataLoader = trainer.get_eval_dataloader()\n",
    "    \n",
    "    total_tokens = 0\n",
    "    topk_matches = 0\n",
    "    rr_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            # batch is a dict of torch.Tensor already\n",
    "            labels = batch.pop(\"labels\").to(device)   # shape (bs, seq_len)\n",
    "            # move all other inputs to device\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            # forward\n",
    "            logits = model(**inputs).logits.cpu().numpy()  # (bs, seq_len, vocab_size)\n",
    "            lbls   = labels.cpu().numpy()                 # (bs, seq_len)\n",
    "\n",
    "            # mask out padding tokens\n",
    "            mask = lbls != -100                             # (bs, seq_len)\n",
    "            flat_logits = logits[mask].reshape(-1, logits.shape[-1])  # (N_toks, V)\n",
    "            flat_labels = lbls[mask].reshape(-1)                     # (N_toks,)\n",
    "\n",
    "            # Top-k matches via argpartition (cheap per-row)\n",
    "            topk_idxs = np.argpartition(flat_logits, -k, axis=-1)[:, -k:]\n",
    "            topk_matches += np.sum([flat_labels[i] in topk_idxs[i]\n",
    "                                    for i in range(flat_labels.shape[0])])\n",
    "\n",
    "            # MRR: rank = 1 + # of logits > true_logit\n",
    "            true_scores = flat_logits[np.arange(flat_labels.shape[0]), flat_labels]\n",
    "            ranks = 1 + np.sum(flat_logits > true_scores[:, None], axis=1)\n",
    "            rr_sum += np.sum(1.0 / ranks)\n",
    "\n",
    "            total_tokens += flat_labels.shape[0]\n",
    "\n",
    "    topk_acc = topk_matches / total_tokens\n",
    "    mrr      = rr_sum / total_tokens\n",
    "    return topk_acc, mrr\n",
    "\n",
    "# Usage:\n",
    "top5_acc, mrr = stream_topk_mrr(model, trainer, k=5)\n",
    "print(f\"Top-5 Accuracy: {top5_acc:.4f}\")\n",
    "print(f\"MRR:             {mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65adcad8",
   "metadata": {},
   "source": [
    "Top-5 Accuracy bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['Top-5 Accuracy'], [top5_acc])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Top-5 Token Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c0f549",
   "metadata": {},
   "source": [
    "MRR bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c873fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.bar(['MRR'], [mrr])\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Mean Reciprocal Rank (MRR)')\n",
    "plt.ylabel('MRR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5c3f3d",
   "metadata": {},
   "source": [
    "Inference Latency Histogram\n",
    "Measure per-sample inference latency and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca58b2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "device = next(model.parameters()).device\n",
    "loader = trainer.get_eval_dataloader()\n",
    "\n",
    "latencies = []\n",
    "with torch.no_grad():\n",
    "    for batch in loader:\n",
    "        batch.pop(\"labels\", None)\n",
    "        inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "        start = time.monotonic()\n",
    "        _ = model(**inputs)\n",
    "        end = time.monotonic()\n",
    "        latencies.append(end - start)\n",
    "\n",
    "latencies = np.array(latencies)\n",
    "\n",
    "print(f\"Mean latency: {latencies.mean():.4f}s\")\n",
    "print(f\"Std  latency: {latencies.std():.4f}s\")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(latencies, bins=20)\n",
    "plt.xlabel(\"Latency (seconds)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Inference Latency Distribution\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
